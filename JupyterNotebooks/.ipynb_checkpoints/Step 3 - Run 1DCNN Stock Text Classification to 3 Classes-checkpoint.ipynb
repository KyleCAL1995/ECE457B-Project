{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kero5\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\kero5\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#############################################################################################################################################\n",
    "#\n",
    "# Stock future performance classification based on text\n",
    "#\n",
    "# Approach:\n",
    "#\n",
    "# Build on top of it a 1D convolutional neural network, ending in a softmax output over 3 even categories.\n",
    "# Use word Glove word vectors for large English text corpus as inputs model\n",
    "#\n",
    "# Steps\n",
    "# 1) After cleaning, we convert all text samples in the dataset into sequences of word indices.  In this case, a \"word index\" would simply be an integer ID for the word. \n",
    "# 2) We consider the top 350,000 most commonly occuring words in the dataset\n",
    "# 3) We truncate the sequences to a maximum length of 25,000 words.\n",
    "# 5) We [repare an \"embedding matrix\" which will contain at index i the embedding vector for the word of index i in our word index.\n",
    "# 6) Then, we load this embedding matrix into a Keras Embedding layer, set to be frozen (its weights, the embedding vectors, will not be updated during training).\n",
    "#\n",
    "###############################################################################################################################################\n",
    "\n",
    "# import libraries\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from six.moves import zip\n",
    "import json\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from pandas import DataFrame   \n",
    "import pickle\n",
    "import re\n",
    "import sys \n",
    "import azureml\n",
    "import string\n",
    "from scipy import stats\n",
    "import pip\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer     \n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense, Input, Flatten \n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding \n",
    "from keras.models import Model \n",
    "from keras.utils import plot_model\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.models import load_model\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.layers import Embedding\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import initializers\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import initializers \n",
    "from keras.layers import regularizers \n",
    "from keras.layers import constraints \n",
    "from keras.layers import Activation\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.constraints import max_norm\n",
    "import keras.backend as K\n",
    "import os\n",
    "import tempfile  \n",
    "import logging\n",
    "import gensim\n",
    "from gensim.models import Phrases, phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models import Word2Vec as wv\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models, similarities\n",
    "from IPython.display import SVG\n",
    "import cloudpickle\n",
    "import csv\n",
    "import mkl\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from keras.models import load_model\n",
    "import re\n",
    "import io\n",
    "from os.path import dirname, join\n",
    "import regex\n",
    "import graphviz\n",
    "import pydotplus\n",
    "import pyparsing\n",
    "from keras.utils import plot_model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1081, 6)\n",
      "Review the unique labels ['neutral' 'negative' 'positive']\n",
      "(1081, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>sample_count</th>\n",
       "      <th>merged_text</th>\n",
       "      <th>weighted_daily_price</th>\n",
       "      <th>price_change_1d</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-10-05</td>\n",
       "      <td>1374</td>\n",
       "      <td>where smart money placing their bets then when...</td>\n",
       "      <td>239.428434</td>\n",
       "      <td>-0.001299</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-03-30</td>\n",
       "      <td>1578</td>\n",
       "      <td>the bitcoin tip bits has been collected youfig...</td>\n",
       "      <td>246.012185</td>\n",
       "      <td>-0.000229</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  sample_count  \\\n",
       "0  2015-10-05          1374   \n",
       "1  2015-03-30          1578   \n",
       "\n",
       "                                         merged_text  weighted_daily_price  \\\n",
       "0  where smart money placing their bets then when...            239.428434   \n",
       "1  the bitcoin tip bits has been collected youfig...            246.012185   \n",
       "\n",
       "   price_change_1d    label  \n",
       "0        -0.001299  neutral  \n",
       "1        -0.000229  neutral  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "##########################################\n",
    "# Get Previously Organized Stock Data\n",
    "##########################################\n",
    "\n",
    "HOME_DIR = 'C:/Users/kero5/Documents/UW/4B/ECE457B/Project'\n",
    "\n",
    "\n",
    "labelled_data = pd.concat([\n",
    "    pd.read_csv(HOME_DIR + '/Data/labelled_data0.csv'),\n",
    "    pd.read_csv(HOME_DIR + '/Data/labelled_data1.csv'),\n",
    "    pd.read_csv(HOME_DIR + '/Data/labelled_data2.csv')\n",
    "])\n",
    "print(labelled_data.shape)\n",
    "\n",
    "#np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "#################################\n",
    "#If necessary, convert categories\n",
    "#################################\n",
    "#thedata['ReturnQuantile'] = thedata['ReturnQuantile'].map({0:0,1:1,2:1,3:1,4:2})\n",
    "print('Review the unique labels',labelled_date['label'].unique())\n",
    "\n",
    "##########################################\n",
    "# clean up the text in the data with regex\n",
    "##########################################\n",
    "\n",
    "def clean_text(row):\n",
    "    # text = row['text'].encode('utf-8').lower()\n",
    "    text = row['merged_text'].lower()\n",
    "\n",
    "    # Remove newline characters\n",
    "    cleantext = text.replace('\\r\\n', ' ')\n",
    "\n",
    "    # Convert HTML punctuation chaaracters\n",
    "    cleantext = cleantext.replace(' www.', ' ')   \n",
    "    cleantext = cleantext.replace('.com ', ' ')    \n",
    "    cleantext = cleantext.replace('.', ' ')\n",
    "    cleantext = cleantext.replace(',', ' ')\n",
    "    cleantext = cleantext.replace('!', ' ')\n",
    "    cleantext = cleantext.replace('$;', ' ')\n",
    "    cleantext = cleantext.replace(';', ' ')\n",
    "    cleantext = cleantext.replace(')', ' ')\n",
    "    cleantext = cleantext.replace('(', ' ')   \n",
    "    cleantext = cleantext.replace('>', ' ')  \n",
    "    cleantext = cleantext.replace('<', ' ')  \n",
    "    cleantext = cleantext.replace('-', ' ')  #take away hyphen and collapse hyphenated words\n",
    "    cleantext = cleantext.replace(' the ', ' ')  \n",
    "    cleantext = cleantext.replace(' of ', ' ')   \n",
    "    cleantext = cleantext.replace(' in ', ' ')  \n",
    "    cleantext = cleantext.replace(' and ', ' ')  \n",
    "    cleantext = cleantext.replace(' by ', ' ')  \n",
    "    cleantext = cleantext.replace(' to ', ' ')  \n",
    "    cleantext = cleantext.replace(' at ', ' ')  \n",
    "    cleantext = cleantext.replace(' on ', ' ') \n",
    "    cleantext = cleantext.replace(' for ', ' ')  \n",
    "    cleantext = cleantext.replace(' be ', ' ')   \n",
    "    cleantext = cleantext.replace(' is ', ' ')    \n",
    "    cleantext = cleantext.replace(' or ', ' ')   \n",
    "    cleantext = cleantext.replace(' we ', ' ')   \n",
    "    cleantext = cleantext.replace(' that ', ' ')   \n",
    "    cleantext = cleantext.replace(' our ', ' ')   \n",
    "    cleantext = cleantext.replace(' as ', ' ')        \n",
    "    cleantext = cleantext.replace(' from ', ' ')   \n",
    "    cleantext = cleantext.replace(' are ', ' ')   \n",
    "    cleantext = cleantext.replace(' with ', ' ')   \n",
    "    cleantext = cleantext.replace(' us ', ' ')   \n",
    "    cleantext = cleantext.replace(' was ', ' ')        \n",
    "    cleantext = cleantext.replace(' this ', ' ')   \n",
    "    cleantext = cleantext.replace(' an ', ' ')        \n",
    "    cleantext = cleantext.replace(' by ', ' ')   \n",
    "    cleantext = cleantext.replace(' sr ', ' ')      \n",
    "    cleantext = cleantext.replace(' it ', ' ')  \n",
    "    cleantext = cleantext.replace(' s ', ' ')\n",
    "    \n",
    "    # added in ECE457B\n",
    "    cleantext = cleantext.replace(' you ', ' ') \n",
    "    cleantext = cleantext.replace(' they ', ' ')   \n",
    "    cleantext = cleantext.replace(' your ', ' ')   \n",
    "\n",
    "    #remove non alpha characters and specific noise\n",
    "    cleantext = re.sub(r'\\d+', ' ',cleantext)\n",
    "    cleantext = re.sub(r'^b',' ',cleantext)\n",
    "    cleantext = re.sub(r'[^\\w]',' ',cleantext)\n",
    "    cleantext = cleantext.replace('xc xs', ' ')  \n",
    "    cleantext = cleantext.replace('xe xs', ' ')  \n",
    "    cleantext = cleantext.replace('xc xS', ' ')  \n",
    "    cleantext = cleantext.replace('xe xS', ' ')  \n",
    "    cleantext = cleantext.replace('xc xa', ' ')  \n",
    "    cleantext = cleantext.replace('xe xa', ' ')  \n",
    "    cleantext = cleantext.replace(' xc xc x', ' ')  \n",
    "    cleantext = cleantext.replace(' xc ', ' ')  \n",
    "    cleantext = cleantext.replace(' xe ', ' ')  \n",
    "    cleantext = cleantext.replace(' xs ', ' ')  \n",
    "    cleantext = cleantext.replace(' xa ', ' ')  \n",
    "    cleantext = cleantext.replace(' ct ', ' ')  \n",
    "    cleantext = cleantext.replace(' x ', ' ')  \n",
    "    cleantext = cleantext.replace(' non exclusive ', ' non-exclusive ') \n",
    "    cleantext = cleantext.replace(' u ', ' ')   \n",
    "    cleantext = cleantext.replace(' s ', ' ')  \n",
    "    \n",
    "    #remove specific noise\n",
    "    cleantext = cleantext.translate(str.maketrans({'‘':' ','’':' '}))\n",
    "    cleantext = cleantext.translate(str.maketrans({',':' ',',':' '}))\n",
    "    cleantext = cleantext.translate(str.maketrans({'[':' ',']':' '}))\n",
    "    cleantext = cleantext.translate(str.maketrans({'\"':' ','%':' '}))\n",
    "    cleantext = cleantext.translate(str.maketrans({'^':' ','*':' '}))\n",
    "\n",
    "    #remove punctuation\n",
    "    punctpattern = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    cleanttext = re.sub(punctpattern,'', cleantext)\n",
    "\n",
    "    #remove single letter word\n",
    "    cleantext = re.sub('(\\\\b[A-Za-z] \\\\b|\\\\b [A-Za-z]\\\\b)', '', cleantext) \n",
    "\n",
    "    # Remove extra spaces\n",
    "    cleantext = re.sub('\\s+', ' ', cleantext).strip()\n",
    "\n",
    "    return cleantext\n",
    "\n",
    "#apply regex fixes to the input text column\n",
    "labelled_data['merged_text'] = labelled_data.apply(clean_text, axis=1)\n",
    "labelled_data.to_csv(HOME_DIR + '//Data//cleaned.csv', sep='\\t', encoding='utf-8')\n",
    "\n",
    "print(labelled_data.shape)\n",
    "labelled_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1081, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>sample_count</th>\n",
       "      <th>merged_text</th>\n",
       "      <th>weighted_daily_price</th>\n",
       "      <th>price_change_1d</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>2015-01-14</td>\n",
       "      <td>3709</td>\n",
       "      <td>here information usaggybagz history has usaggy...</td>\n",
       "      <td>169.483398</td>\n",
       "      <td>-0.348141</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>3434</td>\n",
       "      <td>what qualifications looked through profile doe...</td>\n",
       "      <td>187.067433</td>\n",
       "      <td>0.103751</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>2015-01-16</td>\n",
       "      <td>2124</td>\n",
       "      <td>no oz silver medallion gold accents bearing bi...</td>\n",
       "      <td>195.910316</td>\n",
       "      <td>0.047271</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>2015-01-17</td>\n",
       "      <td>1673</td>\n",
       "      <td>unocoinsnothappy cap wants send bitcoin tip bi...</td>\n",
       "      <td>208.618659</td>\n",
       "      <td>0.064868</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>2015-01-20</td>\n",
       "      <td>2337</td>\n",
       "      <td>the bitcoin tip hug bits has been collected pe...</td>\n",
       "      <td>221.658632</td>\n",
       "      <td>0.037223</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2015-01-21</td>\n",
       "      <td>2234</td>\n",
       "      <td>fast access bitcoins case want make occasional...</td>\n",
       "      <td>223.797728</td>\n",
       "      <td>0.009650</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>2015-01-22</td>\n",
       "      <td>2236</td>\n",
       "      <td>people using bitcoin changetip etcexclusively ...</td>\n",
       "      <td>233.379664</td>\n",
       "      <td>0.042815</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>2015-01-23</td>\n",
       "      <td>1984</td>\n",
       "      <td>just described bitcoin merchants brah what exa...</td>\n",
       "      <td>229.873546</td>\n",
       "      <td>-0.015023</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>2015-01-24</td>\n",
       "      <td>1580</td>\n",
       "      <td>good morning fellow bitcoin traders est januar...</td>\n",
       "      <td>245.171916</td>\n",
       "      <td>0.066551</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2015-01-25</td>\n",
       "      <td>1906</td>\n",
       "      <td>its very design bitcoin makes political statem...</td>\n",
       "      <td>249.245839</td>\n",
       "      <td>0.016617</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>2015-01-26</td>\n",
       "      <td>2312</td>\n",
       "      <td>it based bitcoin works through social media we...</td>\n",
       "      <td>285.981187</td>\n",
       "      <td>0.147386</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2015-01-27</td>\n",
       "      <td>2009</td>\n",
       "      <td>actually need lot less bitcoin than need usd b...</td>\n",
       "      <td>261.647824</td>\n",
       "      <td>-0.085087</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>2015-01-28</td>\n",
       "      <td>2213</td>\n",
       "      <td>hello gates could give some bitcoins education...</td>\n",
       "      <td>248.311221</td>\n",
       "      <td>-0.050972</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>2015-01-29</td>\n",
       "      <td>2206</td>\n",
       "      <td>did try make look like happend earlier history...</td>\n",
       "      <td>233.273915</td>\n",
       "      <td>-0.060558</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>2015-01-30</td>\n",
       "      <td>2004</td>\n",
       "      <td>good rule thumb like all assets physical metal...</td>\n",
       "      <td>233.913616</td>\n",
       "      <td>0.002742</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>2015-01-31</td>\n",
       "      <td>1564</td>\n",
       "      <td>good question argument used those who claim bi...</td>\n",
       "      <td>228.619784</td>\n",
       "      <td>-0.022632</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>2015-02-01</td>\n",
       "      <td>1587</td>\n",
       "      <td>dollar currency lost since created bitcoin sin...</td>\n",
       "      <td>221.423898</td>\n",
       "      <td>-0.031475</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2015-02-02</td>\n",
       "      <td>1922</td>\n",
       "      <td>what options exist investing bitcoin people co...</td>\n",
       "      <td>227.955533</td>\n",
       "      <td>0.029498</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>2015-02-03</td>\n",
       "      <td>1908</td>\n",
       "      <td>think bitcoin good buy right now worse case lo...</td>\n",
       "      <td>236.909178</td>\n",
       "      <td>0.039278</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>2015-02-04</td>\n",
       "      <td>2368</td>\n",
       "      <td>that true certainly weaker than main blockchai...</td>\n",
       "      <td>226.124780</td>\n",
       "      <td>-0.045521</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>2015-02-05</td>\n",
       "      <td>2151</td>\n",
       "      <td>just one besides datamining awesome generosity...</td>\n",
       "      <td>221.427028</td>\n",
       "      <td>-0.020775</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2015-02-06</td>\n",
       "      <td>1828</td>\n",
       "      <td>the catch because bitassets backed bts not und...</td>\n",
       "      <td>221.343844</td>\n",
       "      <td>-0.000376</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>2015-02-07</td>\n",
       "      <td>1857</td>\n",
       "      <td>yeah guys their down over last year let just f...</td>\n",
       "      <td>226.918323</td>\n",
       "      <td>0.025185</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>2015-02-08</td>\n",
       "      <td>1761</td>\n",
       "      <td>guys lesson everybody first notion property us...</td>\n",
       "      <td>225.480951</td>\n",
       "      <td>-0.006334</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-02-09</td>\n",
       "      <td>2049</td>\n",
       "      <td>even core devs couldn operate don control bitc...</td>\n",
       "      <td>221.130366</td>\n",
       "      <td>-0.019295</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2015-02-10</td>\n",
       "      <td>2324</td>\n",
       "      <td>itcoin internet currency reason because intern...</td>\n",
       "      <td>219.017986</td>\n",
       "      <td>-0.009553</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2015-02-11</td>\n",
       "      <td>2083</td>\n",
       "      <td>give few weeks bitcoin will worthless that com...</td>\n",
       "      <td>221.296326</td>\n",
       "      <td>0.010403</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>2015-02-12</td>\n",
       "      <td>1884</td>\n",
       "      <td>edit current full alphabetical list here figur...</td>\n",
       "      <td>220.846479</td>\n",
       "      <td>-0.002033</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>2015-02-13</td>\n",
       "      <td>1841</td>\n",
       "      <td>man those comments have closer turning bullish...</td>\n",
       "      <td>234.209962</td>\n",
       "      <td>0.060510</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>2015-02-14</td>\n",
       "      <td>1467</td>\n",
       "      <td>the aol bitcoin what aol even aol can honestly...</td>\n",
       "      <td>248.842515</td>\n",
       "      <td>0.062476</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>2017-12-02</td>\n",
       "      <td>9824</td>\n",
       "      <td>the following comment usuallycontroversial sil...</td>\n",
       "      <td>10946.056300</td>\n",
       "      <td>0.063807</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>2017-12-03</td>\n",
       "      <td>9616</td>\n",
       "      <td>looks good check out andreas antonopoulos vide...</td>\n",
       "      <td>11317.806360</td>\n",
       "      <td>0.033962</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>2017-12-04</td>\n",
       "      <td>10963</td>\n",
       "      <td>yeah bcash languishing has virtually zero name...</td>\n",
       "      <td>11389.214464</td>\n",
       "      <td>0.006309</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>2017-12-05</td>\n",
       "      <td>11870</td>\n",
       "      <td>bitcoin bill here some reasons why low fees re...</td>\n",
       "      <td>11730.844593</td>\n",
       "      <td>0.029996</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>2017-12-06</td>\n",
       "      <td>15043</td>\n",
       "      <td>ulessiarty received bch usd how to use what is...</td>\n",
       "      <td>12934.987064</td>\n",
       "      <td>0.102648</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>2017-12-07</td>\n",
       "      <td>21778</td>\n",
       "      <td>currency think ethereum will take over bitcoin...</td>\n",
       "      <td>15921.248499</td>\n",
       "      <td>0.230867</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>2017-12-08</td>\n",
       "      <td>20458</td>\n",
       "      <td>keep maintaining investments weedstocks bitcoi...</td>\n",
       "      <td>16148.336141</td>\n",
       "      <td>0.014263</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>2017-12-09</td>\n",
       "      <td>13652</td>\n",
       "      <td>itcointax best resource found allows import es...</td>\n",
       "      <td>15263.356753</td>\n",
       "      <td>-0.054803</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2017-12-10</td>\n",
       "      <td>14086</td>\n",
       "      <td>midwest usa currently paying ball smaller amou...</td>\n",
       "      <td>14840.534690</td>\n",
       "      <td>-0.027702</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>2017-12-11</td>\n",
       "      <td>15111</td>\n",
       "      <td>think sitting verge exponential interest curre...</td>\n",
       "      <td>16677.139814</td>\n",
       "      <td>0.123756</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>2017-12-12</td>\n",
       "      <td>15470</td>\n",
       "      <td>already gone through tsa made power laptop got...</td>\n",
       "      <td>17116.557387</td>\n",
       "      <td>0.026348</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>2017-12-13</td>\n",
       "      <td>13602</td>\n",
       "      <td>yeah but case windfall someone earning stable ...</td>\n",
       "      <td>17003.370575</td>\n",
       "      <td>-0.006613</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2017-12-14</td>\n",
       "      <td>12991</td>\n",
       "      <td>thanks accepting bitcoin cash donations added ...</td>\n",
       "      <td>16754.664503</td>\n",
       "      <td>-0.014627</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>2017-12-15</td>\n",
       "      <td>11792</td>\n",
       "      <td>wonder what will take before others community ...</td>\n",
       "      <td>17500.587430</td>\n",
       "      <td>0.044520</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>2017-12-16</td>\n",
       "      <td>10687</td>\n",
       "      <td>great deal evidence suggests many banks not co...</td>\n",
       "      <td>18767.431192</td>\n",
       "      <td>0.072389</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>2017-12-17</td>\n",
       "      <td>10290</td>\n",
       "      <td>you confused not understanding concept decentr...</td>\n",
       "      <td>19455.628104</td>\n",
       "      <td>0.036670</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>2017-12-18</td>\n",
       "      <td>11901</td>\n",
       "      <td>don think its good idea clicked one captcha go...</td>\n",
       "      <td>18779.790859</td>\n",
       "      <td>-0.034737</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>2017-12-19</td>\n",
       "      <td>13527</td>\n",
       "      <td>here post archival purposes author sendmebitco...</td>\n",
       "      <td>18046.801749</td>\n",
       "      <td>-0.039031</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>2017-12-20</td>\n",
       "      <td>19636</td>\n",
       "      <td>quick example business area china unable compe...</td>\n",
       "      <td>16627.524664</td>\n",
       "      <td>-0.078644</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>2017-12-21</td>\n",
       "      <td>15291</td>\n",
       "      <td>your submission has been flagged removal becau...</td>\n",
       "      <td>16071.712878</td>\n",
       "      <td>-0.033427</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>2017-12-22</td>\n",
       "      <td>19987</td>\n",
       "      <td>able just fine week before all madness happene...</td>\n",
       "      <td>13581.146181</td>\n",
       "      <td>-0.154966</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>2017-12-23</td>\n",
       "      <td>12732</td>\n",
       "      <td>because missing problem same cash people don l...</td>\n",
       "      <td>15082.896041</td>\n",
       "      <td>0.110576</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>2017-12-24</td>\n",
       "      <td>10765</td>\n",
       "      <td>not increasing throuput also suicide either wa...</td>\n",
       "      <td>14139.620217</td>\n",
       "      <td>-0.062539</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>2017-12-25</td>\n",
       "      <td>9486</td>\n",
       "      <td>itcoin cash not bitcoin mean have delusional n...</td>\n",
       "      <td>14146.858539</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>2017-12-26</td>\n",
       "      <td>9761</td>\n",
       "      <td>the funcoin working better than bitcoin today ...</td>\n",
       "      <td>15492.830385</td>\n",
       "      <td>0.095143</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2017-12-27</td>\n",
       "      <td>10331</td>\n",
       "      <td>beautiful possibility solves problem one talki...</td>\n",
       "      <td>15512.080368</td>\n",
       "      <td>0.001243</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>2017-12-28</td>\n",
       "      <td>9898</td>\n",
       "      <td>summary links bitcoin coolest trick its voting...</td>\n",
       "      <td>14278.621269</td>\n",
       "      <td>-0.079516</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>2017-12-29</td>\n",
       "      <td>8939</td>\n",
       "      <td>what can anyone bitcoin address other than sen...</td>\n",
       "      <td>14606.928206</td>\n",
       "      <td>0.022993</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>2017-12-30</td>\n",
       "      <td>9662</td>\n",
       "      <td>please explain what real bitcoin one currency ...</td>\n",
       "      <td>13467.995751</td>\n",
       "      <td>-0.077972</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>8097</td>\n",
       "      <td>they want make money don care about technologi...</td>\n",
       "      <td>13471.106092</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1081 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  sample_count  \\\n",
       "186  2015-01-14          3709   \n",
       "78   2015-01-15          3434   \n",
       "172  2015-01-16          2124   \n",
       "314  2015-01-17          1673   \n",
       "133  2015-01-20          2337   \n",
       "62   2015-01-21          2234   \n",
       "127  2015-01-22          2236   \n",
       "316  2015-01-23          1984   \n",
       "320  2015-01-24          1580   \n",
       "99   2015-01-25          1906   \n",
       "92   2015-01-26          2312   \n",
       "62   2015-01-27          2009   \n",
       "128  2015-01-28          2213   \n",
       "118  2015-01-29          2206   \n",
       "126  2015-01-30          2004   \n",
       "226  2015-01-31          1564   \n",
       "299  2015-02-01          1587   \n",
       "96   2015-02-02          1922   \n",
       "223  2015-02-03          1908   \n",
       "203  2015-02-04          2368   \n",
       "238  2015-02-05          2151   \n",
       "44   2015-02-06          1828   \n",
       "233  2015-02-07          1857   \n",
       "305  2015-02-08          1761   \n",
       "2    2015-02-09          2049   \n",
       "29   2015-02-10          2324   \n",
       "11   2015-02-11          2083   \n",
       "147  2015-02-12          1884   \n",
       "287  2015-02-13          1841   \n",
       "162  2015-02-14          1467   \n",
       "..          ...           ...   \n",
       "336  2017-12-02          9824   \n",
       "139  2017-12-03          9616   \n",
       "213  2017-12-04         10963   \n",
       "169  2017-12-05         11870   \n",
       "346  2017-12-06         15043   \n",
       "317  2017-12-07         21778   \n",
       "121  2017-12-08         20458   \n",
       "276  2017-12-09         13652   \n",
       "59   2017-12-10         14086   \n",
       "355  2017-12-11         15111   \n",
       "340  2017-12-12         15470   \n",
       "238  2017-12-13         13602   \n",
       "49   2017-12-14         12991   \n",
       "298  2017-12-15         11792   \n",
       "261  2017-12-16         10687   \n",
       "190  2017-12-17         10290   \n",
       "251  2017-12-18         11901   \n",
       "316  2017-12-19         13527   \n",
       "211  2017-12-20         19636   \n",
       "278  2017-12-21         15291   \n",
       "288  2017-12-22         19987   \n",
       "329  2017-12-23         12732   \n",
       "371  2017-12-24         10765   \n",
       "132  2017-12-25          9486   \n",
       "117  2017-12-26          9761   \n",
       "58   2017-12-27         10331   \n",
       "239  2017-12-28          9898   \n",
       "278  2017-12-29          8939   \n",
       "285  2017-12-30          9662   \n",
       "69   2017-12-31          8097   \n",
       "\n",
       "                                           merged_text  weighted_daily_price  \\\n",
       "186  here information usaggybagz history has usaggy...            169.483398   \n",
       "78   what qualifications looked through profile doe...            187.067433   \n",
       "172  no oz silver medallion gold accents bearing bi...            195.910316   \n",
       "314  unocoinsnothappy cap wants send bitcoin tip bi...            208.618659   \n",
       "133  the bitcoin tip hug bits has been collected pe...            221.658632   \n",
       "62   fast access bitcoins case want make occasional...            223.797728   \n",
       "127  people using bitcoin changetip etcexclusively ...            233.379664   \n",
       "316  just described bitcoin merchants brah what exa...            229.873546   \n",
       "320  good morning fellow bitcoin traders est januar...            245.171916   \n",
       "99   its very design bitcoin makes political statem...            249.245839   \n",
       "92   it based bitcoin works through social media we...            285.981187   \n",
       "62   actually need lot less bitcoin than need usd b...            261.647824   \n",
       "128  hello gates could give some bitcoins education...            248.311221   \n",
       "118  did try make look like happend earlier history...            233.273915   \n",
       "126  good rule thumb like all assets physical metal...            233.913616   \n",
       "226  good question argument used those who claim bi...            228.619784   \n",
       "299  dollar currency lost since created bitcoin sin...            221.423898   \n",
       "96   what options exist investing bitcoin people co...            227.955533   \n",
       "223  think bitcoin good buy right now worse case lo...            236.909178   \n",
       "203  that true certainly weaker than main blockchai...            226.124780   \n",
       "238  just one besides datamining awesome generosity...            221.427028   \n",
       "44   the catch because bitassets backed bts not und...            221.343844   \n",
       "233  yeah guys their down over last year let just f...            226.918323   \n",
       "305  guys lesson everybody first notion property us...            225.480951   \n",
       "2    even core devs couldn operate don control bitc...            221.130366   \n",
       "29   itcoin internet currency reason because intern...            219.017986   \n",
       "11   give few weeks bitcoin will worthless that com...            221.296326   \n",
       "147  edit current full alphabetical list here figur...            220.846479   \n",
       "287  man those comments have closer turning bullish...            234.209962   \n",
       "162  the aol bitcoin what aol even aol can honestly...            248.842515   \n",
       "..                                                 ...                   ...   \n",
       "336  the following comment usuallycontroversial sil...          10946.056300   \n",
       "139  looks good check out andreas antonopoulos vide...          11317.806360   \n",
       "213  yeah bcash languishing has virtually zero name...          11389.214464   \n",
       "169  bitcoin bill here some reasons why low fees re...          11730.844593   \n",
       "346  ulessiarty received bch usd how to use what is...          12934.987064   \n",
       "317  currency think ethereum will take over bitcoin...          15921.248499   \n",
       "121  keep maintaining investments weedstocks bitcoi...          16148.336141   \n",
       "276  itcointax best resource found allows import es...          15263.356753   \n",
       "59   midwest usa currently paying ball smaller amou...          14840.534690   \n",
       "355  think sitting verge exponential interest curre...          16677.139814   \n",
       "340  already gone through tsa made power laptop got...          17116.557387   \n",
       "238  yeah but case windfall someone earning stable ...          17003.370575   \n",
       "49   thanks accepting bitcoin cash donations added ...          16754.664503   \n",
       "298  wonder what will take before others community ...          17500.587430   \n",
       "261  great deal evidence suggests many banks not co...          18767.431192   \n",
       "190  you confused not understanding concept decentr...          19455.628104   \n",
       "251  don think its good idea clicked one captcha go...          18779.790859   \n",
       "316  here post archival purposes author sendmebitco...          18046.801749   \n",
       "211  quick example business area china unable compe...          16627.524664   \n",
       "278  your submission has been flagged removal becau...          16071.712878   \n",
       "288  able just fine week before all madness happene...          13581.146181   \n",
       "329  because missing problem same cash people don l...          15082.896041   \n",
       "371  not increasing throuput also suicide either wa...          14139.620217   \n",
       "132  itcoin cash not bitcoin mean have delusional n...          14146.858539   \n",
       "117  the funcoin working better than bitcoin today ...          15492.830385   \n",
       "58   beautiful possibility solves problem one talki...          15512.080368   \n",
       "239  summary links bitcoin coolest trick its voting...          14278.621269   \n",
       "278  what can anyone bitcoin address other than sen...          14606.928206   \n",
       "285  please explain what real bitcoin one currency ...          13467.995751   \n",
       "69   they want make money don care about technologi...          13471.106092   \n",
       "\n",
       "     price_change_1d     label  \n",
       "186        -0.348141  negative  \n",
       "78          0.103751  positive  \n",
       "172         0.047271  positive  \n",
       "314         0.064868  positive  \n",
       "133         0.037223  positive  \n",
       "62          0.009650   neutral  \n",
       "127         0.042815  positive  \n",
       "316        -0.015023  negative  \n",
       "320         0.066551  positive  \n",
       "99          0.016617   neutral  \n",
       "92          0.147386  positive  \n",
       "62         -0.085087  negative  \n",
       "128        -0.050972  negative  \n",
       "118        -0.060558  negative  \n",
       "126         0.002742   neutral  \n",
       "226        -0.022632  negative  \n",
       "299        -0.031475  negative  \n",
       "96          0.029498  positive  \n",
       "223         0.039278  positive  \n",
       "203        -0.045521  negative  \n",
       "238        -0.020775  negative  \n",
       "44         -0.000376   neutral  \n",
       "233         0.025185  positive  \n",
       "305        -0.006334   neutral  \n",
       "2          -0.019295  negative  \n",
       "29         -0.009553   neutral  \n",
       "11          0.010403   neutral  \n",
       "147        -0.002033   neutral  \n",
       "287         0.060510  positive  \n",
       "162         0.062476  positive  \n",
       "..               ...       ...  \n",
       "336         0.063807  positive  \n",
       "139         0.033962  positive  \n",
       "213         0.006309   neutral  \n",
       "169         0.029996  positive  \n",
       "346         0.102648  positive  \n",
       "317         0.230867  positive  \n",
       "121         0.014263   neutral  \n",
       "276        -0.054803  negative  \n",
       "59         -0.027702  negative  \n",
       "355         0.123756  positive  \n",
       "340         0.026348  positive  \n",
       "238        -0.006613   neutral  \n",
       "49         -0.014627  negative  \n",
       "298         0.044520  positive  \n",
       "261         0.072389  positive  \n",
       "190         0.036670  positive  \n",
       "251        -0.034737  negative  \n",
       "316        -0.039031  negative  \n",
       "211        -0.078644  negative  \n",
       "278        -0.033427  negative  \n",
       "288        -0.154966  negative  \n",
       "329         0.110576  positive  \n",
       "371        -0.062539  negative  \n",
       "132         0.000512   neutral  \n",
       "117         0.095143  positive  \n",
       "58          0.001243   neutral  \n",
       "239        -0.079516  negative  \n",
       "278         0.022993  positive  \n",
       "285        -0.077972  negative  \n",
       "69          0.000231   neutral  \n",
       "\n",
       "[1081 rows x 6 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(labelled_data.shape)\n",
    "labelled_data.sort_values(by=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post regex justcleandocs\n",
      " 0    where smart money placing their bets then when...\n",
      "1    the bitcoin tip bits has been collected youfig...\n",
      "2    funny how interview viabtc about need bigger b...\n",
      "3    i bot bleep bloop someone has linked thread an...\n",
      "4    just spent half year kenya have say pesa every...\n",
      "5    fidelity mining bitcoins collaborates coinbase...\n",
      "6    the ones subscribe currently let talk bitcoin ...\n",
      "7    probably just copying layout evo because many ...\n",
      "8    hey there cheyenne sweet petite kinky camshows...\n",
      "9    guess will see end year bitcoin died like time...\n",
      "Name: merged_text, dtype: object\n",
      "head of just labels\n",
      "       label\n",
      "0   neutral\n",
      "1   neutral\n",
      "2  negative\n",
      "3   neutral\n",
      "4   neutral\n"
     ]
    }
   ],
   "source": [
    "################################\n",
    "# Convert labels to categorical\n",
    "################################\n",
    "justcleandocs = labelled_data['merged_text']\n",
    "print('post regex justcleandocs\\n',justcleandocs.head(10))\n",
    "\n",
    "justlabels=pd.DataFrame(labelled_data['label'])\n",
    "print('head of just labels\\n',justlabels.head(5))\n",
    "\n",
    "###################################################\n",
    "# Set Global Vars\n",
    "####################################################\n",
    "MAX_SEQUENCE_LENGTH = 10000\n",
    "MAX_NB_WORDS = 400000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.2\n",
    "LEARNING_RATE = .00011\n",
    "BATCH_SIZE = 32\n",
    "DROPOUT_RATE = 0.45\n",
    "INNERLAYER_DROPOUT_RATE = 0.15\n",
    "np.random.seed(2032)\n",
    "\n",
    "#change directory to write results\n",
    "GLOVE_DIR = HOME_DIR + '/glove/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_index {0: 0, 1: 1, 2: 2}\n",
      "Found 949794 unique tokens\n",
      "Shape of data tensor:  (1081, 10000)\n",
      "Shape of label tensor:  (1081, 3)\n",
      "length of y_val 216\n",
      "shape of y_val (216, 3)\n",
      "length of X_val 216\n",
      "shape of X_val (216, 10000)\n",
      "test and training sets saved to disk for later evaluation\n"
     ]
    }
   ],
   "source": [
    "\n",
    "######################################################\n",
    "# Format our text samples and labels for use in Keras\n",
    "######################################################\n",
    "# Then we can format our text samples and labels into tensors that can be fed into a neural network. \n",
    "# Here we tokenize our source 'justcleandocs'\n",
    "# note that the values here are ultimately indexes to the actual words\n",
    "\n",
    "#convert text format\n",
    "justcleandocslist  = justcleandocs.values\n",
    "justcleandocslist[6]\n",
    "\n",
    "# convert labels to int\n",
    "label_id_mapping = {'negative':0, 'neutral':1, 'positive':2}\n",
    "labels  = justlabels.replace({'label': label_id_mapping}).values\n",
    "\n",
    "labels_index = {}\n",
    "labels_index =  {0:0,1:1,2:2}\n",
    "print('labels_index', labels_index)\n",
    "\n",
    "#tokenize the text\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(justcleandocslist) #tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(justcleandocslist) #sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index #word_index = tokenizer.word_index\n",
    "print('Found {} unique tokens'.format(len(word_index)))\n",
    "#print('sequences first', sequences[0])\n",
    "\n",
    "#Pad sequences so that they all have the same length in a batch of input data \n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='pre', truncating='pre')\n",
    "sequences = None\n",
    "texts = None\n",
    "\n",
    "\n",
    "##################################################\n",
    "#build label array from target y label in data set\n",
    "##################################################\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor: ', data.shape)\n",
    "print('Shape of label tensor: ', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "X_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "X_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "print('length of y_val',len(y_val))\n",
    "print('shape of y_val',y_val.shape)\n",
    "print('length of X_val',len(X_val))\n",
    "print('shape of X_val',X_val.shape)\n",
    "\n",
    "#os.chdir('C:\\\\glove\\\\nextagenda')\n",
    "\n",
    "#from itertools import islice\n",
    "#head = list(islice(y_val, 6))\n",
    "#print('head of yval',head)\n",
    "\n",
    "#####################################\n",
    "# Save Validation Set for Evaluation\n",
    "####################################\n",
    "np.savetxt('y_val_3bin.txt', y_val, delimiter=',')\n",
    "np.savetxt('X_val_3bin.txt', X_val,  fmt='%s', delimiter=',')\n",
    "print('test and training sets saved to disk for later evaluation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors to prepare the embedding layer...\n",
      "C:\\Users\\kero5\\Documents\\UW\\4B\\ECE457B\\Project\\JupyterNotebooks\n",
      "Loading Glove Model...\n",
      "   0         1         2         3         4         5         6         7    \\\n",
      "0  the  0.046560  0.213180 -0.007436 -0.458540 -0.035639  0.236430 -0.288360   \n",
      "1    , -0.255390 -0.257230  0.131690 -0.042688  0.218170 -0.022702 -0.178540   \n",
      "2    . -0.125590  0.013630  0.103060 -0.101230  0.098128  0.136270 -0.107210   \n",
      "3   of -0.076947 -0.021211  0.212710 -0.722320 -0.139880 -0.122340 -0.175210   \n",
      "4   to -0.257560 -0.057132 -0.671900 -0.380820 -0.364210 -0.082155 -0.010955   \n",
      "\n",
      "        8         9      ...          291       292       293       294  \\\n",
      "0  0.215210 -0.134860    ...    -0.013064 -0.296860 -0.079913  0.195000   \n",
      "1  0.107560  0.058936    ...     0.075968 -0.014359 -0.073794  0.221760   \n",
      "2  0.236970  0.328700    ...     0.060148 -0.156190 -0.119490  0.234450   \n",
      "3  0.121370 -0.070866    ...    -0.366730 -0.386030  0.302900  0.015747   \n",
      "4 -0.082047  0.460560    ...    -0.012806 -0.597070  0.317340 -0.252670   \n",
      "\n",
      "        295       296       297       298       299       300  \n",
      "0  0.031549  0.285060 -0.087461  0.009061 -0.209890  0.053913  \n",
      "1  0.146520  0.566860  0.053307 -0.232900 -0.122260  0.354990  \n",
      "2  0.081367  0.246180 -0.152420 -0.342240 -0.022394  0.136840  \n",
      "3  0.340360  0.478410  0.068617  0.183510 -0.291830 -0.046533  \n",
      "4  0.543840  0.063007 -0.049795 -0.160430  0.046744 -0.070621  \n",
      "\n",
      "[5 rows x 301 columns]\n",
      "shape of glove model (400000, 301)\n",
      "wordkeys type of file <class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kero5\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 399998 word vectors.\n",
      "Building Embedding Matrix...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "########################################\n",
    "# Preparing the embedding layer\n",
    "########################################\n",
    "\n",
    "#load in word vectors from glove reference global English data set\n",
    "# https://nlp.stanford.edu/projects/glove/\n",
    "# see more reference links at bottom\n",
    "\n",
    "print('Loading word vectors to prepare the embedding layer...')\n",
    "print(os.getcwd())\n",
    "\n",
    "embeddings_index = {}\n",
    "print('Loading Glove Model...')\n",
    "gloveFile = GLOVE_DIR + '/glove.6B.300d.txt'\n",
    "words = pd.read_table(gloveFile, sep=\" \", header=None, quoting=csv.QUOTE_NONE)\n",
    "\n",
    "print(words.head(5))\n",
    "print('shape of glove model',words.shape)\n",
    "\n",
    "wordkeys=words.iloc[:,0]\n",
    "print('wordkeys type of file', type(wordkeys))\n",
    "words2 = words.rename(columns={ words.columns[0]: \"words\" })\n",
    "words2['words'].apply(str)\n",
    "#print(words2.dtypes)\n",
    "\n",
    "embeddings_index = words2.set_index('words').T.to_dict('list')\n",
    "\n",
    "#print(dict(list(embeddings_index.items())[0:2]))\n",
    "print('Found {} word vectors.'.format(len(embeddings_index)))\n",
    "#usage of pandas function dataFrame.to_dict(outtype='dict') outtype : str {‘dict’, ‘list’, ‘series’}\n",
    "\n",
    "\n",
    "#################################\n",
    "#Build the embedding matrix\n",
    "#################################\n",
    "\n",
    "print('Building Embedding Matrix...')\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 1D Convnet with global maxpooling\n",
      "Shape of training data sample tensor:  (865, 10000)\n",
      "Shape of training label tensor:  (865, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##############################################\n",
    "#Training a 1D convnet\n",
    "##############################################\n",
    "\n",
    "print('Train 1D Convnet with global maxpooling')\n",
    "print('Shape of training data sample tensor: ', X_train.shape)\n",
    "print('Shape of training label tensor: ', y_train.shape)\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "x = Conv1D(128, 5, activation='elu', kernel_initializer='lecun_uniform')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Dropout(INNERLAYER_DROPOUT_RATE)(x)\n",
    "\n",
    "x = Conv1D(128, 5, activation='elu', kernel_initializer='lecun_uniform')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Dropout(INNERLAYER_DROPOUT_RATE)(x)\n",
    "\n",
    "x = Conv1D(128, 5, activation='elu', kernel_initializer='lecun_uniform')(x)\n",
    "x = MaxPooling1D(35)(x)  # global max pooling\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(100, activation='elu', kernel_initializer='lecun_uniform')(x) # best initializers: #glorot_normal #VarianceScaling #lecun_uniform\n",
    "x = Dropout(DROPOUT_RATE)(x)\n",
    "\n",
    "preds = Dense(len(labels_index), activation='softmax')(x) #no initialization in output layer\n",
    "\n",
    "\n",
    "model = Model(sequence_input, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 865 samples, validate on 216 samples\n",
      "Epoch 1/5\n",
      "865/865 [==============================] - 288s 333ms/step - loss: 1.0677 - acc: 0.5318 - val_loss: 1.0251 - val_acc: 0.5324\n",
      "Epoch 2/5\n",
      "865/865 [==============================] - 282s 326ms/step - loss: 1.0439 - acc: 0.5410 - val_loss: 1.0290 - val_acc: 0.5324\n",
      "Epoch 3/5\n",
      "865/865 [==============================] - 264s 305ms/step - loss: 1.0756 - acc: 0.5318 - val_loss: 1.0422 - val_acc: 0.5324\n",
      "Epoch 4/5\n",
      "865/865 [==============================] - 284s 328ms/step - loss: 1.0331 - acc: 0.5295 - val_loss: 1.0329 - val_acc: 0.5324\n",
      "Epoch 5/5\n",
      "252/865 [=======>......................] - ETA: 3:17 - loss: 1.0485 - acc: 0.5079"
     ]
    }
   ],
   "source": [
    "################################\n",
    "#Compile model, set optimizers\n",
    "################################\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 10000\n",
    "MAX_NB_WORDS = 400000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.2\n",
    "LEARNING_RATE = .00011\n",
    "BATCH_SIZE = 36\n",
    "DROPOUT_RATE = 0.45\n",
    "INNERLAYER_DROPOUT_RATE = 0.15\n",
    "\n",
    "\n",
    "adam = optimizers.Adam(lr=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0, clipvalue=0.5)#, clipnorm=1.)\n",
    "rmsprop = optimizers.RMSprop(lr=LEARNING_RATE, rho=0.9, epsilon=1e-08, decay=0.00)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer= adam,\n",
    "              metrics=['accuracy'])\n",
    "from keras.callbacks import History \n",
    "history = History()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30)\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=5,\n",
    "          validation_data=(X_val, y_val), callbacks=[early_stopping, history])\n",
    "\n",
    "\n",
    "##############################\n",
    "# Save Model and Plots\n",
    "##############################\n",
    "model.save(GLOVE_DIR+'tockText_3Level3EvenClass_modelNov2_8pm.h5')\n",
    " \n",
    "import matplotlib.pyplot as plt  \n",
    "plt.figure(1)  \n",
    "\n",
    "# summarize history for accuracy  \n",
    "plt.subplot(211)  \n",
    "plt.plot(history.history['acc'])  \n",
    "plt.plot(history.history['val_acc'])  \n",
    "plt.title('model accuracy')  \n",
    "plt.ylabel('accuracy')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='upper left')  \n",
    "   \n",
    "# summarize history for loss  \n",
    "plt.subplot(212)  \n",
    "plt.plot(history.history['loss'])  \n",
    "plt.plot(history.history['val_loss'])  \n",
    "plt.title('model loss')  \n",
    "plt.ylabel('loss')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='upper left')  \n",
    "plt.show()  \n",
    "\n",
    "plot_model(model, to_file=GLOVE_DIR+'\\stocktext_model3class.png')\n",
    "\n",
    "#from IPython.display import SVG\n",
    "#from keras.utils.vis_utils import model_to_dot\n",
    "#SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "\n",
    "##############################\n",
    "# More helpful links\n",
    "##############################\n",
    "\n",
    "#We can also test how well we would have performed by not using pre-trained word embeddings, \n",
    "#but instead initializing our Embedding layer from scratch and learning its weights during training. \n",
    "\n",
    "#https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "#https://arxiv.org/abs/1603.03827 \n",
    "#https://nlp.stanford.edu/projects/glove/ \n",
    "#https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python\n",
    "#https://stackoverflow.com/questions/27139908/load-precomputed-vectors-gensim?rq=1\n",
    "#https://stackoverflow.com/questions/14415741/numpy-array-vs-asarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Train on 283 samples, validate on 70 samples\n",
    "Epoch 1/24\n",
    "283/283 [==============================] - 87s 309ms/step - loss: 1.3329 - acc: 0.4664 - val_loss: 1.0088 - val_acc: 0.5571\n",
    "Epoch 2/24\n",
    "283/283 [==============================] - 85s 302ms/step - loss: 1.1907 - acc: 0.3781 - val_loss: 1.0115 - val_acc: 0.5571\n",
    "Epoch 3/24\n",
    "283/283 [==============================] - 81s 287ms/step - loss: 1.1227 - acc: 0.5265 - val_loss: 0.9900 - val_acc: 0.5571\n",
    "Epoch 4/24\n",
    "283/283 [==============================] - 79s 281ms/step - loss: 1.0947 - acc: 0.4417 - val_loss: 0.9886 - val_acc: 0.5571\n",
    "Epoch 5/24\n",
    "283/283 [==============================] - 77s 273ms/step - loss: 1.0927 - acc: 0.5018 - val_loss: 1.0153 - val_acc: 0.5571\n",
    "Epoch 6/24\n",
    "283/283 [==============================] - 77s 273ms/step - loss: 1.0709 - acc: 0.4735 - val_loss: 0.9918 - val_acc: 0.5571\n",
    "Epoch 7/24\n",
    "283/283 [==============================] - 80s 282ms/step - loss: 1.0672 - acc: 0.4806 - val_loss: 1.0154 - val_acc: 0.5571\n",
    "Epoch 8/24\n",
    "283/283 [==============================] - 80s 283ms/step - loss: 1.0179 - acc: 0.5265 - val_loss: 0.9844 - val_acc: 0.5571\n",
    "Epoch 9/24\n",
    "283/283 [==============================] - 80s 282ms/step - loss: 1.0337 - acc: 0.5194 - val_loss: 0.9978 - val_acc: 0.5571\n",
    "Epoch 10/24\n",
    "283/283 [==============================] - 78s 274ms/step - loss: 1.0491 - acc: 0.5230 - val_loss: 1.0129 - val_acc: 0.5571\n",
    "Epoch 11/24\n",
    "283/283 [==============================] - 84s 297ms/step - loss: 0.9958 - acc: 0.5159 - val_loss: 1.0044 - val_acc: 0.5571\n",
    "Epoch 12/24\n",
    "283/283 [==============================] - 91s 323ms/step - loss: 1.0032 - acc: 0.5300 - val_loss: 1.0214 - val_acc: 0.5571\n",
    "Epoch 13/24\n",
    "283/283 [==============================] - 80s 284ms/step - loss: 0.9750 - acc: 0.5406 - val_loss: 0.9861 - val_acc: 0.5571\n",
    "Epoch 14/24\n",
    "283/283 [==============================] - 86s 304ms/step - loss: 1.0104 - acc: 0.5124 - val_loss: 1.0156 - val_acc: 0.5571\n",
    "Epoch 15/24\n",
    "283/283 [==============================] - 97s 341ms/step - loss: 1.0091 - acc: 0.5371 - val_loss: 1.0117 - val_acc: 0.5571\n",
    "Epoch 16/24\n",
    "283/283 [==============================] - 93s 328ms/step - loss: 1.0359 - acc: 0.4346 - val_loss: 0.9911 - val_acc: 0.5571\n",
    "Epoch 17/24\n",
    "283/283 [==============================] - 87s 307ms/step - loss: 1.0218 - acc: 0.5371 - val_loss: 1.0388 - val_acc: 0.5429\n",
    "Epoch 18/24\n",
    "128/283 [============>.................] - ETA: 41s - loss: 0.9980 - acc: 0.5312"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

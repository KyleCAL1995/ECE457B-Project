{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kero5\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\kero5\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#############################################################################################################################################\n",
    "#\n",
    "# Stock future performance classification based on text\n",
    "#\n",
    "# Approach:\n",
    "#\n",
    "# Build on top of it a 1D convolutional neural network, ending in a softmax output over 3 even categories.\n",
    "# Use word Glove word vectors for large English text corpus as inputs model\n",
    "#\n",
    "# Steps\n",
    "# 1) After cleaning, we convert all text samples in the dataset into sequences of word indices.  In this case, a \"word index\" would simply be an integer ID for the word. \n",
    "# 2) We consider the top 350,000 most commonly occuring words in the dataset\n",
    "# 3) We truncate the sequences to a maximum length of 25,000 words.\n",
    "# 5) We [repare an \"embedding matrix\" which will contain at index i the embedding vector for the word of index i in our word index.\n",
    "# 6) Then, we load this embedding matrix into a Keras Embedding layer, set to be frozen (its weights, the embedding vectors, will not be updated during training).\n",
    "#\n",
    "###############################################################################################################################################\n",
    "\n",
    "# import libraries\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from six.moves import zip\n",
    "import json\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from pandas import DataFrame   \n",
    "import pickle\n",
    "import re\n",
    "import sys \n",
    "import azureml\n",
    "import string\n",
    "from scipy import stats\n",
    "import pip\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer     \n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense, Input, Flatten \n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding \n",
    "from keras.models import Model \n",
    "from keras.utils import plot_model\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.models import load_model\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.layers import Embedding\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import initializers\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import initializers \n",
    "from keras.layers import regularizers \n",
    "from keras.layers import constraints \n",
    "from keras.layers import Activation\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.constraints import max_norm\n",
    "import keras.backend as K\n",
    "import os\n",
    "import tempfile  \n",
    "import logging\n",
    "import gensim\n",
    "from gensim.models import Phrases, phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models import Word2Vec as wv\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models, similarities\n",
    "from IPython.display import SVG\n",
    "import cloudpickle\n",
    "import csv\n",
    "import mkl\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from keras.models import load_model\n",
    "import re\n",
    "import io\n",
    "from os.path import dirname, join\n",
    "import regex\n",
    "import graphviz\n",
    "import pydotplus\n",
    "import pyparsing\n",
    "from keras.utils import plot_model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(353, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>sample_count</th>\n",
       "      <th>merged_text</th>\n",
       "      <th>weighted_daily_price</th>\n",
       "      <th>price_change_1d</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-10-05</td>\n",
       "      <td>1374</td>\n",
       "      <td>where smart money placing their bets then when...</td>\n",
       "      <td>239.428434</td>\n",
       "      <td>-0.001299</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-03-30</td>\n",
       "      <td>1578</td>\n",
       "      <td>the bitcoin tip bits has been collected youfig...</td>\n",
       "      <td>246.012185</td>\n",
       "      <td>-0.000229</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  sample_count  \\\n",
       "0  2015-10-05          1374   \n",
       "1  2015-03-30          1578   \n",
       "\n",
       "                                         merged_text  weighted_daily_price  \\\n",
       "0  where smart money placing their bets then when...            239.428434   \n",
       "1  the bitcoin tip bits has been collected youfig...            246.012185   \n",
       "\n",
       "   price_change_1d    label  \n",
       "0        -0.001299  neutral  \n",
       "1        -0.000229  neutral  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "##########################################\n",
    "# Get Previously Organized Stock Data\n",
    "##########################################\n",
    "\n",
    "HOME_DIR = 'C:/Users/kero5/Documents/UW/4B/ECE457B/Project'\n",
    "\n",
    "labelled_data = pd.read_csv(HOME_DIR + '/Data/labelled_data0.csv')\n",
    "labelled_data.append(pd.read_csv(HOME_DIR + '/Data/labelled_data1.csv', header=0))\n",
    "labelled_data.append(pd.read_csv(HOME_DIR + '/Data/labelled_data2.csv', header=0))\n",
    "\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "#################################\n",
    "#If necessary, convert categories\n",
    "#################################\n",
    "#thedata['ReturnQuantile'] = thedata['ReturnQuantile'].map({0:0,1:1,2:1,3:1,4:2})\n",
    "print('Review the unique labels',labelled_date['label'].unique())\n",
    "\n",
    "##########################################\n",
    "# clean up the text in the data with regex\n",
    "##########################################\n",
    "\n",
    "def clean_text(row):\n",
    "    # text = row['text'].encode('utf-8').lower()\n",
    "    text = row['merged_text'].lower()\n",
    "\n",
    "    # Remove newline characters\n",
    "    cleantext = text.replace('\\r\\n', ' ')\n",
    "\n",
    "    # Convert HTML punctuation chaaracters\n",
    "    cleantext = cleantext.replace(' www.', ' ')   \n",
    "    cleantext = cleantext.replace('.com ', ' ')    \n",
    "    cleantext = cleantext.replace('.', ' ')\n",
    "    cleantext = cleantext.replace(',', ' ')\n",
    "    cleantext = cleantext.replace('!', ' ')\n",
    "    cleantext = cleantext.replace('$;', ' ')\n",
    "    cleantext = cleantext.replace(';', ' ')\n",
    "    cleantext = cleantext.replace(')', ' ')\n",
    "    cleantext = cleantext.replace('(', ' ')   \n",
    "    cleantext = cleantext.replace('>', ' ')  \n",
    "    cleantext = cleantext.replace('<', ' ')  \n",
    "    cleantext = cleantext.replace('-', ' ')  #take away hyphen and collapse hyphenated words\n",
    "    cleantext = cleantext.replace(' the ', ' ')  \n",
    "    cleantext = cleantext.replace(' of ', ' ')   \n",
    "    cleantext = cleantext.replace(' in ', ' ')  \n",
    "    cleantext = cleantext.replace(' and ', ' ')  \n",
    "    cleantext = cleantext.replace(' by ', ' ')  \n",
    "    cleantext = cleantext.replace(' to ', ' ')  \n",
    "    cleantext = cleantext.replace(' at ', ' ')  \n",
    "    cleantext = cleantext.replace(' on ', ' ') \n",
    "    cleantext = cleantext.replace(' for ', ' ')  \n",
    "    cleantext = cleantext.replace(' be ', ' ')   \n",
    "    cleantext = cleantext.replace(' is ', ' ')    \n",
    "    cleantext = cleantext.replace(' or ', ' ')   \n",
    "    cleantext = cleantext.replace(' we ', ' ')   \n",
    "    cleantext = cleantext.replace(' that ', ' ')   \n",
    "    cleantext = cleantext.replace(' our ', ' ')   \n",
    "    cleantext = cleantext.replace(' as ', ' ')        \n",
    "    cleantext = cleantext.replace(' from ', ' ')   \n",
    "    cleantext = cleantext.replace(' are ', ' ')   \n",
    "    cleantext = cleantext.replace(' with ', ' ')   \n",
    "    cleantext = cleantext.replace(' us ', ' ')   \n",
    "    cleantext = cleantext.replace(' was ', ' ')        \n",
    "    cleantext = cleantext.replace(' this ', ' ')   \n",
    "    cleantext = cleantext.replace(' an ', ' ')        \n",
    "    cleantext = cleantext.replace(' by ', ' ')   \n",
    "    cleantext = cleantext.replace(' sr ', ' ')      \n",
    "    cleantext = cleantext.replace(' it ', ' ')  \n",
    "    cleantext = cleantext.replace(' s ', ' ')\n",
    "    \n",
    "    # added in ECE457B\n",
    "    cleantext = cleantext.replace(' you ', ' ') \n",
    "    cleantext = cleantext.replace(' they ', ' ')   \n",
    "    cleantext = cleantext.replace(' your ', ' ')   \n",
    "\n",
    "    #remove non alpha characters and specific noise\n",
    "    cleantext = re.sub(r'\\d+', ' ',cleantext)\n",
    "    cleantext = re.sub(r'^b',' ',cleantext)\n",
    "    cleantext = re.sub(r'[^\\w]',' ',cleantext)\n",
    "    cleantext = cleantext.replace('xc xs', ' ')  \n",
    "    cleantext = cleantext.replace('xe xs', ' ')  \n",
    "    cleantext = cleantext.replace('xc xS', ' ')  \n",
    "    cleantext = cleantext.replace('xe xS', ' ')  \n",
    "    cleantext = cleantext.replace('xc xa', ' ')  \n",
    "    cleantext = cleantext.replace('xe xa', ' ')  \n",
    "    cleantext = cleantext.replace(' xc xc x', ' ')  \n",
    "    cleantext = cleantext.replace(' xc ', ' ')  \n",
    "    cleantext = cleantext.replace(' xe ', ' ')  \n",
    "    cleantext = cleantext.replace(' xs ', ' ')  \n",
    "    cleantext = cleantext.replace(' xa ', ' ')  \n",
    "    cleantext = cleantext.replace(' ct ', ' ')  \n",
    "    cleantext = cleantext.replace(' x ', ' ')  \n",
    "    cleantext = cleantext.replace(' non exclusive ', ' non-exclusive ') \n",
    "    cleantext = cleantext.replace(' u ', ' ')   \n",
    "    cleantext = cleantext.replace(' s ', ' ')  \n",
    "    \n",
    "    #remove specific noise\n",
    "    cleantext = cleantext.translate(str.maketrans({'‘':' ','’':' '}))\n",
    "    cleantext = cleantext.translate(str.maketrans({',':' ',',':' '}))\n",
    "    cleantext = cleantext.translate(str.maketrans({'[':' ',']':' '}))\n",
    "    cleantext = cleantext.translate(str.maketrans({'\"':' ','%':' '}))\n",
    "    cleantext = cleantext.translate(str.maketrans({'^':' ','*':' '}))\n",
    "\n",
    "    #remove punctuation\n",
    "    punctpattern = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    cleanttext = re.sub(punctpattern,'', cleantext)\n",
    "\n",
    "    #remove single letter word\n",
    "    cleantext = re.sub('(\\\\b[A-Za-z] \\\\b|\\\\b [A-Za-z]\\\\b)', '', cleantext) \n",
    "\n",
    "    # Remove extra spaces\n",
    "    cleantext = re.sub('\\s+', ' ', cleantext).strip()\n",
    "\n",
    "    return cleantext\n",
    "\n",
    "#apply regex fixes to the input text column\n",
    "labelled_data['merged_text'] = labelled_data.apply(clean_text, axis=1)\n",
    "labelled_data.to_csv(HOME_DIR + '//Data//cleaned.csv', sep='\\t', encoding='utf-8')\n",
    "\n",
    "print(labelled_data.shape)\n",
    "labelled_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post regex justcleandocs\n",
      " 0    where smart money placing their bets then when...\n",
      "1    the bitcoin tip bits has been collected youfig...\n",
      "2    funny how interview viabtc about need bigger b...\n",
      "3    i bot bleep bloop someone has linked thread an...\n",
      "4    just spent half year kenya have say pesa every...\n",
      "5    fidelity mining bitcoins collaborates coinbase...\n",
      "6    the ones subscribe currently let talk bitcoin ...\n",
      "7    probably just copying layout evo because many ...\n",
      "8    hey there cheyenne sweet petite kinky camshows...\n",
      "9    guess will see end year bitcoin died like time...\n",
      "Name: merged_text, dtype: object\n",
      "head of just labels\n",
      "       label\n",
      "0   neutral\n",
      "1   neutral\n",
      "2  negative\n",
      "3   neutral\n",
      "4   neutral\n"
     ]
    }
   ],
   "source": [
    "################################\n",
    "# Convert labels to categorical\n",
    "################################\n",
    "justcleandocs = labelled_data['merged_text']\n",
    "print('post regex justcleandocs\\n',justcleandocs.head(10))\n",
    "\n",
    "justlabels=pd.DataFrame(labelled_data['label'])\n",
    "print('head of just labels\\n',justlabels.head(5))\n",
    "\n",
    "###################################################\n",
    "# Set Global Vars\n",
    "####################################################\n",
    "MAX_SEQUENCE_LENGTH = 10000\n",
    "MAX_NB_WORDS = 400000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.2\n",
    "LEARNING_RATE = .00011\n",
    "BATCH_SIZE = 32\n",
    "DROPOUT_RATE = 0.45\n",
    "INNERLAYER_DROPOUT_RATE = 0.15\n",
    "np.random.seed(2032)\n",
    "\n",
    "#change directory to write results\n",
    "GLOVE_DIR = HOME_DIR + '/glove/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_index {0: 0, 1: 1, 2: 2}\n",
      "Found 449026 unique tokens\n",
      "Shape of data tensor:  (353, 10000)\n",
      "Shape of label tensor:  (353, 3)\n",
      "length of y_val 70\n",
      "shape of y_val (70, 3)\n",
      "length of X_val 70\n",
      "shape of X_val (70, 10000)\n",
      "test and training sets saved to disk for later evaluation\n"
     ]
    }
   ],
   "source": [
    "\n",
    "######################################################\n",
    "# Format our text samples and labels for use in Keras\n",
    "######################################################\n",
    "# Then we can format our text samples and labels into tensors that can be fed into a neural network. \n",
    "# Here we tokenize our source 'justcleandocs'\n",
    "# note that the values here are ultimately indexes to the actual words\n",
    "\n",
    "#convert text format\n",
    "justcleandocslist  = justcleandocs.values\n",
    "justcleandocslist[6]\n",
    "\n",
    "# convert labels to int\n",
    "label_id_mapping = {'negative':0, 'neutral':1, 'positive':2}\n",
    "labels  = justlabels.replace({'label': label_id_mapping}).values\n",
    "\n",
    "labels_index = {}\n",
    "labels_index =  {0:0,1:1,2:2}\n",
    "print('labels_index', labels_index)\n",
    "\n",
    "#tokenize the text\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(justcleandocslist) #tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(justcleandocslist) #sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index #word_index = tokenizer.word_index\n",
    "print('Found {} unique tokens'.format(len(word_index)))\n",
    "#print('sequences first', sequences[0])\n",
    "\n",
    "#Pad sequences so that they all have the same length in a batch of input data \n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='pre', truncating='pre')\n",
    "sequences = None\n",
    "texts = None\n",
    "\n",
    "\n",
    "##################################################\n",
    "#build label array from target y label in data set\n",
    "##################################################\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor: ', data.shape)\n",
    "print('Shape of label tensor: ', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "X_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "X_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "print('length of y_val',len(y_val))\n",
    "print('shape of y_val',y_val.shape)\n",
    "print('length of X_val',len(X_val))\n",
    "print('shape of X_val',X_val.shape)\n",
    "\n",
    "#os.chdir('C:\\\\glove\\\\nextagenda')\n",
    "\n",
    "#from itertools import islice\n",
    "#head = list(islice(y_val, 6))\n",
    "#print('head of yval',head)\n",
    "\n",
    "\n",
    "\n",
    "#####################################\n",
    "# Save Validation Set for Evaluation\n",
    "####################################\n",
    "np.savetxt('y_val_3bin.txt', y_val, delimiter=',')\n",
    "np.savetxt('X_val_3bin.txt', X_val,  fmt='%s', delimiter=',')\n",
    "print('test and training sets saved to disk for later evaluation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors to prepare the embedding layer...\n",
      "C:\\Users\\kero5\\Documents\\UW\\4B\\ECE457B\\Project\\JupyterNotebooks\n",
      "Loading Glove Model...\n",
      "   0         1         2         3         4         5         6         7    \\\n",
      "0  the  0.046560  0.213180 -0.007436 -0.458540 -0.035639  0.236430 -0.288360   \n",
      "1    , -0.255390 -0.257230  0.131690 -0.042688  0.218170 -0.022702 -0.178540   \n",
      "2    . -0.125590  0.013630  0.103060 -0.101230  0.098128  0.136270 -0.107210   \n",
      "3   of -0.076947 -0.021211  0.212710 -0.722320 -0.139880 -0.122340 -0.175210   \n",
      "4   to -0.257560 -0.057132 -0.671900 -0.380820 -0.364210 -0.082155 -0.010955   \n",
      "\n",
      "        8         9      ...          291       292       293       294  \\\n",
      "0  0.215210 -0.134860    ...    -0.013064 -0.296860 -0.079913  0.195000   \n",
      "1  0.107560  0.058936    ...     0.075968 -0.014359 -0.073794  0.221760   \n",
      "2  0.236970  0.328700    ...     0.060148 -0.156190 -0.119490  0.234450   \n",
      "3  0.121370 -0.070866    ...    -0.366730 -0.386030  0.302900  0.015747   \n",
      "4 -0.082047  0.460560    ...    -0.012806 -0.597070  0.317340 -0.252670   \n",
      "\n",
      "        295       296       297       298       299       300  \n",
      "0  0.031549  0.285060 -0.087461  0.009061 -0.209890  0.053913  \n",
      "1  0.146520  0.566860  0.053307 -0.232900 -0.122260  0.354990  \n",
      "2  0.081367  0.246180 -0.152420 -0.342240 -0.022394  0.136840  \n",
      "3  0.340360  0.478410  0.068617  0.183510 -0.291830 -0.046533  \n",
      "4  0.543840  0.063007 -0.049795 -0.160430  0.046744 -0.070621  \n",
      "\n",
      "[5 rows x 301 columns]\n",
      "shape of glove model (400000, 301)\n",
      "wordkeys type of file <class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kero5\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 399998 word vectors.\n",
      "Building Embedding Matrix...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "########################################\n",
    "# Preparing the embedding layer\n",
    "########################################\n",
    "\n",
    "#load in word vectors from glove reference global English data set\n",
    "# https://nlp.stanford.edu/projects/glove/\n",
    "# see more reference links at bottom\n",
    "\n",
    "print('Loading word vectors to prepare the embedding layer...')\n",
    "print(os.getcwd())\n",
    "\n",
    "embeddings_index = {}\n",
    "print('Loading Glove Model...')\n",
    "gloveFile = GLOVE_DIR + '/glove.6B.300d.txt'\n",
    "words = pd.read_table(gloveFile, sep=\" \", header=None, quoting=csv.QUOTE_NONE)\n",
    "\n",
    "print(words.head(5))\n",
    "print('shape of glove model',words.shape)\n",
    "\n",
    "wordkeys=words.iloc[:,0]\n",
    "print('wordkeys type of file', type(wordkeys))\n",
    "words2 = words.rename(columns={ words.columns[0]: \"words\" })\n",
    "words2['words'].apply(str)\n",
    "#print(words2.dtypes)\n",
    "\n",
    "embeddings_index = words2.set_index('words').T.to_dict('list')\n",
    "\n",
    "#print(dict(list(embeddings_index.items())[0:2]))\n",
    "print('Found {} word vectors.'.format(len(embeddings_index)))\n",
    "#usage of pandas function dataFrame.to_dict(outtype='dict') outtype : str {‘dict’, ‘list’, ‘series’}\n",
    "\n",
    "\n",
    "#################################\n",
    "#Build the embedding matrix\n",
    "#################################\n",
    "\n",
    "print('Building Embedding Matrix...')\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 1D Convnet with global maxpooling\n",
      "Shape of training data sample tensor:  (283, 10000)\n",
      "Shape of training label tensor:  (283, 3)\n",
      "WARNING:tensorflow:From C:\\Users\\kero5\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##############################################\n",
    "#Training a 1D convnet\n",
    "##############################################\n",
    "\n",
    "print('Train 1D Convnet with global maxpooling')\n",
    "print('Shape of training data sample tensor: ', X_train.shape)\n",
    "print('Shape of training label tensor: ', y_train.shape)\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "x = Conv1D(128, 5, activation='elu', kernel_initializer='lecun_uniform')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Dropout(INNERLAYER_DROPOUT_RATE)(x)\n",
    "\n",
    "x = Conv1D(128, 5, activation='elu', kernel_initializer='lecun_uniform')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Dropout(INNERLAYER_DROPOUT_RATE)(x)\n",
    "\n",
    "x = Conv1D(128, 5, activation='elu', kernel_initializer='lecun_uniform')(x)\n",
    "x = MaxPooling1D(35)(x)  # global max pooling\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(100, activation='elu', kernel_initializer='lecun_uniform')(x) # best initializers: #glorot_normal #VarianceScaling #lecun_uniform\n",
    "x = Dropout(DROPOUT_RATE)(x)\n",
    "\n",
    "preds = Dense(len(labels_index), activation='softmax')(x) #no initialization in output layer\n",
    "\n",
    "\n",
    "model = Model(sequence_input, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 283 samples, validate on 70 samples\n",
      "Epoch 1/24\n",
      "120/283 [===========>..................] - ETA: 47s - loss: 1.8171 - acc: 0.4083"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-a1c3947138ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m           validation_data=(X_val, y_val), callbacks=[early_stopping, history])\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1235\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1236\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1140\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1321\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1312\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[0;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m             status, run_metadata)\n\u001b[0m\u001b[0;32m   1421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "################################\n",
    "#Compile model, set optimizers\n",
    "################################\n",
    "\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 10000\n",
    "MAX_NB_WORDS = 400000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.2\n",
    "LEARNING_RATE = .00001\n",
    "BATCH_SIZE = 8\n",
    "DROPOUT_RATE = 0.45\n",
    "INNERLAYER_DROPOUT_RATE = 0.15\n",
    "\n",
    "\n",
    "\n",
    "adam = optimizers.Adam(lr=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0, clipvalue=0.5)#, clipnorm=1.)\n",
    "rmsprop = optimizers.RMSprop(lr=LEARNING_RATE, rho=0.9, epsilon=1e-08, decay=0.00)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer= adam,\n",
    "              metrics=['accuracy'])\n",
    "from keras.callbacks import History \n",
    "history = History()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30)\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=24,\n",
    "          validation_data=(X_val, y_val), callbacks=[early_stopping, history])\n",
    "\n",
    "\n",
    "##############################\n",
    "# Save Model and Plots\n",
    "##############################\n",
    "model.save('C:\\\\glove\\\\StockText_3Level3EvenClass_modelNov2_8pm.h5')\n",
    " \n",
    "import matplotlib.pyplot as plt  \n",
    "plt.figure(1)  \n",
    "\n",
    "# summarize history for accuracy  \n",
    "plt.subplot(211)  \n",
    "plt.plot(history.history['acc'])  \n",
    "plt.plot(history.history['val_acc'])  \n",
    "plt.title('model accuracy')  \n",
    "plt.ylabel('accuracy')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='upper left')  \n",
    "   \n",
    "# summarize history for loss  \n",
    "plt.subplot(212)  \n",
    "plt.plot(history.history['loss'])  \n",
    "plt.plot(history.history['val_loss'])  \n",
    "plt.title('model loss')  \n",
    "plt.ylabel('loss')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='upper left')  \n",
    "plt.show()  \n",
    "\n",
    "#plot_model(model, to_file='C:\\\\glove\\stocktext_model3class.png')\n",
    "\n",
    "#from IPython.display import SVG\n",
    "#from keras.utils.vis_utils import model_to_dot\n",
    "#SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "\n",
    "##############################\n",
    "# More helpful links\n",
    "##############################\n",
    "\n",
    "#We can also test how well we would have performed by not using pre-trained word embeddings, \n",
    "#but instead initializing our Embedding layer from scratch and learning its weights during training. \n",
    "\n",
    "#https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "#https://arxiv.org/abs/1603.03827 \n",
    "#https://nlp.stanford.edu/projects/glove/ \n",
    "#https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python\n",
    "#https://stackoverflow.com/questions/27139908/load-precomputed-vectors-gensim?rq=1\n",
    "#https://stackoverflow.com/questions/14415741/numpy-array-vs-asarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Train on 283 samples, validate on 70 samples\n",
    "Epoch 1/24\n",
    "283/283 [==============================] - 87s 309ms/step - loss: 1.3329 - acc: 0.4664 - val_loss: 1.0088 - val_acc: 0.5571\n",
    "Epoch 2/24\n",
    "283/283 [==============================] - 85s 302ms/step - loss: 1.1907 - acc: 0.3781 - val_loss: 1.0115 - val_acc: 0.5571\n",
    "Epoch 3/24\n",
    "283/283 [==============================] - 81s 287ms/step - loss: 1.1227 - acc: 0.5265 - val_loss: 0.9900 - val_acc: 0.5571\n",
    "Epoch 4/24\n",
    "283/283 [==============================] - 79s 281ms/step - loss: 1.0947 - acc: 0.4417 - val_loss: 0.9886 - val_acc: 0.5571\n",
    "Epoch 5/24\n",
    "283/283 [==============================] - 77s 273ms/step - loss: 1.0927 - acc: 0.5018 - val_loss: 1.0153 - val_acc: 0.5571\n",
    "Epoch 6/24\n",
    "283/283 [==============================] - 77s 273ms/step - loss: 1.0709 - acc: 0.4735 - val_loss: 0.9918 - val_acc: 0.5571\n",
    "Epoch 7/24\n",
    "283/283 [==============================] - 80s 282ms/step - loss: 1.0672 - acc: 0.4806 - val_loss: 1.0154 - val_acc: 0.5571\n",
    "Epoch 8/24\n",
    "283/283 [==============================] - 80s 283ms/step - loss: 1.0179 - acc: 0.5265 - val_loss: 0.9844 - val_acc: 0.5571\n",
    "Epoch 9/24\n",
    "283/283 [==============================] - 80s 282ms/step - loss: 1.0337 - acc: 0.5194 - val_loss: 0.9978 - val_acc: 0.5571\n",
    "Epoch 10/24\n",
    "283/283 [==============================] - 78s 274ms/step - loss: 1.0491 - acc: 0.5230 - val_loss: 1.0129 - val_acc: 0.5571\n",
    "Epoch 11/24\n",
    "283/283 [==============================] - 84s 297ms/step - loss: 0.9958 - acc: 0.5159 - val_loss: 1.0044 - val_acc: 0.5571\n",
    "Epoch 12/24\n",
    "283/283 [==============================] - 91s 323ms/step - loss: 1.0032 - acc: 0.5300 - val_loss: 1.0214 - val_acc: 0.5571\n",
    "Epoch 13/24\n",
    "283/283 [==============================] - 80s 284ms/step - loss: 0.9750 - acc: 0.5406 - val_loss: 0.9861 - val_acc: 0.5571\n",
    "Epoch 14/24\n",
    "283/283 [==============================] - 86s 304ms/step - loss: 1.0104 - acc: 0.5124 - val_loss: 1.0156 - val_acc: 0.5571\n",
    "Epoch 15/24\n",
    "283/283 [==============================] - 97s 341ms/step - loss: 1.0091 - acc: 0.5371 - val_loss: 1.0117 - val_acc: 0.5571\n",
    "Epoch 16/24\n",
    "283/283 [==============================] - 93s 328ms/step - loss: 1.0359 - acc: 0.4346 - val_loss: 0.9911 - val_acc: 0.5571\n",
    "Epoch 17/24\n",
    "283/283 [==============================] - 87s 307ms/step - loss: 1.0218 - acc: 0.5371 - val_loss: 1.0388 - val_acc: 0.5429\n",
    "Epoch 18/24\n",
    "128/283 [============>.................] - ETA: 41s - loss: 0.9980 - acc: 0.5312"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.3 |Anaconda custom (64-bit)| (default, Oct 15 2017, 03:27:45) [MSC v.1900 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kero5\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\kero5\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Use AzureML's data collector to log various metrics!\n",
    "#from azureml.logging import current_scriptrun\n",
    "#logger = current_scriptrun()\n",
    "# Use AzureML's data collector to log various metrics!\n",
    "#from azureml.logging import current_scriptrun\n",
    "#logger = current_scriptrun()\n",
    "\n",
    "import string, re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models, similarities\n",
    "import azureml\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from azure.storage.blob import BlockBlobService\n",
    "\n",
    "# import libraries\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from six.moves import zip\n",
    "import json\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from pandas import DataFrame   \n",
    "import pickle\n",
    "import re\n",
    "import sys \n",
    "import azureml\n",
    "import string\n",
    "from scipy import stats\n",
    "import pip\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer     \n",
    "from keras.preprocessing import sequence\n",
    "import os\n",
    "import tempfile  \n",
    "import logging\n",
    "import gensim\n",
    "from gensim.models import Phrases, phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models import Word2Vec as wv\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models, similarities\n",
    "from IPython.display import SVG\n",
    "import cloudpickle\n",
    "import csv\n",
    "import mkl\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from keras.models import load_model\n",
    "import re\n",
    "import io\n",
    "from os.path import dirname, join\n",
    "import regex\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models, similarities\n",
    "import azureml\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import source text and write to a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'C:/Users/kero5/Documents/UW/4B/ECE457B/StockPerformanceClassification//Data//text0.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-ae12073e3e0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mbitcoin_prices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhome_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'//Data//labels.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtexts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhome_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'//Data//text0.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhome_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'//Data//text1.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhome_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'//Data//text2.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 818\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    819\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1047\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1049\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1050\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1694\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1695\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1697\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'C:/Users/kero5/Documents/UW/4B/ECE457B/StockPerformanceClassification//Data//text0.csv' does not exist"
     ]
    }
   ],
   "source": [
    "import os\n",
    "home_dir = 'C:/Users/kero5/Documents/UW/4B/ECE457B/StockPerformanceClassification'\n",
    "bitcoin_prices = pd.read_csv(home_dir + '/Data/labels.csv')\n",
    "\n",
    "texts = pd.read_csv(home_dir + '/Data/text0.csv')\n",
    "texts.append(pd.read_csv(home_dir + '/Data/text1.csv', header=0))\n",
    "texts.append(pd.read_csv(home_dir + '/Data/text2.csv', header=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts_subset = texts.sample(10000)\n",
    "\n",
    "\n",
    "def clean_text(row):\n",
    "    # text = row['text'].decode('utf-8').lower()\n",
    "    text = row['text'].lower()\n",
    "\n",
    "    # Remove newline characters\n",
    "    cleantext = text.replace('\\r\\n', ' ')\n",
    "\n",
    "    # Convert HTML punctuation chaaracters\n",
    "    cleantext = cleantext.replace('.', '')\n",
    "\n",
    "    #remove non alpha characters and specific noise\n",
    "    #cleantext = re.sub(r'\\d+', '',cleantext)\n",
    "    cleantext = re.sub(r'^b','',cleantext)\n",
    "    cleantext = re.sub(r'[^\\w]',' ',cleantext)\n",
    "\n",
    "    #remove specific noise\n",
    "    cleantext = cleantext.translate(str.maketrans({'‘':' ','’':' '}))\n",
    "    cleantext = cleantext.translate(str.maketrans({',':' ',',':' '}))\n",
    "    cleantext = cleantext.translate(str.maketrans({'\"':' ','%':' '}))\n",
    "\n",
    "    #remove punctuation\n",
    "    punctpattern = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    cleanttext = re.sub(punctpattern,'', cleantext)\n",
    "\n",
    "    #remove single letter word\n",
    "    cleantext = re.sub('(\\\\b[A-Za-z] \\\\b|\\\\b [A-Za-z]\\\\b)', '', cleantext) \n",
    "\n",
    "    # Remove extra spaces\n",
    "    cleantext = re.sub('\\s+', ' ', cleantext).strip()\n",
    "\n",
    "    return cleantext\n",
    "\n",
    "#apply regex fixes to the input text column\n",
    "texts_subset['text'] = texts.apply(clean_text, axis=1)\n",
    "texts_subset.to_csv(home_dir + '//Data//cleaned_subset.csv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kero5\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# tokenize, create seqs, pad\n",
    "tok = Tokenizer(num_words=5000, lower=True, split=\" \")\n",
    "tok.fit_on_texts(texts_subset['text'])\n",
    "tok.texts_to_sequences(texts_subset['text'])\n",
    "\n",
    "import nltk \n",
    "\n",
    "nltk.download('punkt')\n",
    "sent_lst = []\n",
    "\n",
    "for doc in texts_subset['text']:\n",
    "    sentences = nltk.tokenize.sent_tokenize(doc)\n",
    "    for sent in sentences:\n",
    "        word_lst = [w for w in nltk.tokenize.word_tokenize(sent) if w.isalnum()]\n",
    "        sent_lst.append(word_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrases<28 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Phrases\n",
    "phrases = Phrases(texts_subset)\n",
    "print(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(33988 unique tokens: ['0001', '03', '053', '09', '2017']...)\n"
     ]
    }
   ],
   "source": [
    "# Create dictionary and save it\n",
    "dictionary = corpora.Dictionary(sent_lst)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(33988 unique tokens: ['0001', '03', '053', '09', '2017']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary.save('texts.dict')\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create tokenized corpus and save it\n",
    "corpus = [dictionary.doc2bow(sent_lst) for sent_lst in sent_lst]\n",
    "corpora.MmCorpus.serialize('texts.dict', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 3), (11, 1), (12, 1), (13, 1), (14, 3), (15, 1), (16, 1), (17, 1), (18, 3), (19, 1), (20, 1), (21, 2), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 2), (31, 1), (32, 1), (33, 1), (34, 2), (35, 1), (36, 1), (37, 1), (38, 2), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1)]\n",
      "[(10, 1), (19, 2), (21, 2), (32, 1), (41, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 2), (55, 1), (56, 1), (57, 2), (58, 1), (59, 2), (60, 1), (61, 2), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 3), (71, 1), (72, 1), (73, 6), (74, 1), (75, 1), (76, 1), (77, 2), (78, 1), (79, 1), (80, 1), (81, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(corpus[0])\n",
    "print(corpus[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create TFIDF model and save it\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "tfidf.save('texts_tfidf.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(10, 0.008181919965673117), (17, 0.089619061838476), (19, 0.041367566682498115), (32, 0.04149302578221466), (41, 0.061635295367656016), (61, 0.03589931448516985), (65, 0.03504135326955593), (73, 0.03441229896268722), (75, 0.058236025001948875), (122, 0.11397208017010206), (123, 0.2693725037467842), (124, 0.18778141651060992), (125, 0.41552366103772814), (126, 0.16746868048690405), (127, 0.06098388537922856), (128, 0.11884948692237803), (129, 0.12527647227064978), (130, 0.1023546129398983), (131, 0.19208889980225496), (132, 0.18510751695572206), (133, 0.38170746741251854), (134, 0.10808592168272757), (135, 0.2923023145818878), (136, 0.2818077929735965), (137, 0.24644269291168058), (138, 0.1813528605975232), (139, 0.3421450570367719), (140, 0.08919217763495967)]\n"
     ]
    }
   ],
   "source": [
    "# Convert document texts to TFIDF\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "print(corpus_tfidf[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract LDA topics, using 1 pass and updating once every 1 chunk (1000 documents)\n",
    "lda = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=20, update_every=1, chunksize=1000, passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6,\n",
       "  '0.017*\"de\" + 0.011*\"bitcointalkorg\" + 0.010*\"indexphp\" + 0.010*\"topic\" + 0.007*\"la\" + 0.005*\"se\" + 0.005*\"https\" + 0.004*\"libertarian\" + 0.004*\"con\" + 0.004*\"da\"'),\n",
       " (12,\n",
       "  '0.028*\"bitcoin\" + 0.019*\"to\" + 0.019*\"is\" + 0.015*\"and\" + 0.014*\"it\" + 0.013*\"cash\" + 0.012*\"the\" + 0.011*\"about\" + 0.010*\"https\" + 0.010*\"you\"'),\n",
       " (9,\n",
       "  '0.030*\"and\" + 0.019*\"my\" + 0.019*\"for\" + 0.017*\"to\" + 0.017*\"you\" + 0.014*\"me\" + 0.014*\"of\" + 0.013*\"with\" + 0.013*\"10\" + 0.011*\"or\"'),\n",
       " (1,\n",
       "  '0.052*\"http\" + 0.038*\"comments\" + 0.023*\"amp\" + 0.022*\"wwwredditcom\" + 0.019*\"npredditcom\" + 0.019*\"to\" + 0.016*\"message\" + 0.016*\"000\" + 0.015*\"borrow\" + 0.014*\"original\"'),\n",
       " (19,\n",
       "  '0.009*\"buttcoin\" + 0.008*\"wu\" + 0.007*\"http\" + 0.006*\"fullz\" + 0.006*\"wow\" + 0.006*\"uk\" + 0.005*\"city\" + 0.004*\"yahoo\" + 0.004*\"table\" + 0.004*\"election\"'),\n",
       " (11,\n",
       "  '0.047*\"he\" + 0.028*\"his\" + 0.020*\"monero\" + 0.008*\"bitcoin\" + 0.007*\"dash\" + 0.007*\"who\" + 0.006*\"him\" + 0.006*\"guy\" + 0.005*\"xrp\" + 0.005*\"got\"'),\n",
       " (2,\n",
       "  '0.066*\"comments\" + 0.061*\"https\" + 0.060*\"the\" + 0.058*\"comment\" + 0.057*\"to\" + 0.055*\"here\" + 0.048*\"wwwredditcom\" + 0.034*\"original\" + 0.033*\"link\" + 0.032*\"bitcoin\"'),\n",
       " (8,\n",
       "  '0.014*\"052\" + 0.006*\"en\" + 0.006*\"104\" + 0.005*\"du\" + 0.004*\"imagesofnetwork\" + 0.004*\"virtacoin\" + 0.004*\"on\" + 0.004*\"bitcoin\" + 0.004*\"der\" + 0.004*\"die\"'),\n",
       " (15,\n",
       "  '0.060*\"the\" + 0.032*\"to\" + 0.030*\"of\" + 0.029*\"is\" + 0.023*\"and\" + 0.020*\"that\" + 0.019*\"in\" + 0.017*\"bitcoin\" + 0.017*\"it\" + 0.012*\"be\"'),\n",
       " (17,\n",
       "  '0.068*\"you\" + 0.042*\"to\" + 0.027*\"your\" + 0.023*\"bitcoin\" + 0.019*\"and\" + 0.019*\"the\" + 0.016*\"if\" + 0.016*\"for\" + 0.014*\"can\" + 0.013*\"or\"')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create index using LDA document projections\n",
    "corpus_lda = lda[corpus]\n",
    "index_lda = similarities.docsim.Similarity('index_lda.dat', corpus_lda, num_features=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity index with 10000 documents in 0 shards (stored under index_lda.dat)\n"
     ]
    }
   ],
   "source": [
    "print(index_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.9999997615814209),\n",
       " (2, 0.97441738843917847),\n",
       " (3, 0.95716357231140137),\n",
       " (4, 0.93029320240020752),\n",
       " (5, 0.91589397192001343),\n",
       " (6, 0.90863794088363647),\n",
       " (7, 0.90827977657318115),\n",
       " (8, 0.89623022079467773),\n",
       " (0, 0.87198424339294434),\n",
       " (44, 0.79476922750473022),\n",
       " (43, 0.75726127624511719),\n",
       " (143, 0.67256861925125122),\n",
       " (151, 0.6539844274520874),\n",
       " (253, 0.61475139856338501),\n",
       " (149, 0.6128806471824646),\n",
       " (150, 0.60356354713439941),\n",
       " (260, 0.60280239582061768),\n",
       " (255, 0.60185325145721436),\n",
       " (259, 0.59907078742980957),\n",
       " (258, 0.59543067216873169)]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return top 5 similar documents\n",
    "index_lda.num_best = 20\n",
    "index_lda[corpus_lda[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 0.061849914), (3, 0.023748983), (6, 0.034551308), (9, 0.03117472), (12, 0.82117105), (17, 0.02108199)]\n"
     ]
    }
   ],
   "source": [
    "print(corpus_lda[171])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.04191737), (2, 0.012401357), (7, 0.40914482), (9, 0.0822838), (10, 0.011953827), (12, 0.042901862), (13, 0.011948517), (17, 0.38173416)]\n"
     ]
    }
   ],
   "source": [
    "print(corpus_lda[128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create index using TFIDF document weights\n",
    "index_tfidf = similarities.docsim.Similarity('index_tfidf.dat', corpus_tfidf, num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.99999922513961792),\n",
       " (8, 0.86761742830276489),\n",
       " (4, 0.70173543691635132),\n",
       " (5, 0.70086997747421265),\n",
       " (6, 0.70060157775878906)]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return top 5 similar documents\n",
    "index_tfidf.num_best = 5\n",
    "index_tfidf[corpus_tfidf[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'texts_subsets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-c8b3c3ed8db0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts_subsets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m172\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'texts_subsets' is not defined"
     ]
    }
   ],
   "source": [
    "print(texts_subsets[172])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(documents[173])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
